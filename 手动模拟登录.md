```
import scrapy
import undetected_chromedriver


class JobboleSpider(scrapy.Spider):
    name = 'jobbole'
    allowed_domains = ['news.cnblogs.com']
    start_urls = ['http://news.cnblogs.com/']
    custom_settings = {
        'COOKIES_ENABLED': True
    }

    # 覆盖默认的方法
    def start_requests(self):
        # 入口可以模拟登录拿到cookie, selenium控制浏览器会被一些网站识别出来
        # 开源项目，防止被识别 https://github.com/ultrafunkamsterdam/undetected-chromedriver
        # 安装 selenium (pip3 install selenium)
        # 安装 undetected-chromedriver (pip install undetected-chromedriver)
        import undetected_chromedriver.v2 as uc
        browser = uc.Chrome()
        browser.get('https://account.cnblogs.com/signin')
        input('回车继续：')
        cookies = browser.get_cookies()
        cookie_dict = {}
        for cookie in cookies:
            cookie_dict[cookie['name']] = cookie['value']

        for url in self.start_urls:
            # 将cookie交给scrapy，那么后续的请求会沿用之前请求的cookie吗，不会。需要加一个参数custom_settings
            # 为了不让浏览器识别再加个headers
            headers = {
                'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'
            }

            yield scrapy.Request('https://news.cnblogs.com/n/701572/', cookies=cookie_dict, headers=headers, dont_filter=True)

    # 手动登录拿到cookie，放入到scrapy
    def parse(self, response):
        # 提取元素 xpath css选择器
        url = response.xpath('//div[@id="news_list"]//h2[@class="news_entry"]/a/@href').extract()
        pass

```